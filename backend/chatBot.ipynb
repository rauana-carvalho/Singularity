{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDNGMC9hPYgnvbTTBDpkZ2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rauana-carvalho/Singularity/blob/main/chatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import unicodedata\n",
        "import re\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import pandas as pd\n",
        "\n",
        "# Lê o arquivo CSV\n",
        "file_path = '/content/testerelatorios.csv'  # Substitua pelo caminho do seu arquivo CSV\n",
        "data = pd.read_csv(file_path, delimiter=',')\n",
        "\n",
        "\n",
        "print(\"Nomes das colunas:\")\n",
        "print(data.columns.tolist())\n",
        "\n",
        "# Function to query the GPT-2 model\n",
        "def query_gpt2(question):\n",
        "    input_ids = tokenizer.encode(question, return_tensors='pt')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(input_ids, max_length=50)\n",
        "\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return response.strip()\n",
        "\n",
        "def normalize_string(text):\n",
        "    text = text.lower().strip()\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')  # Remove accents\n",
        "    return text\n",
        "\n",
        "def remove_duplicate_spaces(text):\n",
        "    return re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "def fetch_experiment_data(question_input):\n",
        "    response = []\n",
        "\n",
        "    question = remove_duplicate_spaces(normalize_string(question_input))\n",
        "\n",
        "    # Dictionary mapping keywords to dataframe columns\n",
        "    mapping = {\n",
        "        \"experiment name\": \"Source_Name\",\n",
        "        \"sample name\": \"Sample_Name\",\n",
        "        \"organism\": \"Characteristics_Organism\",\n",
        "        \"strain\": \"Characteristics_Strain\",\n",
        "        \"laboratory\": \"Characteristics_Animal_Source\",\n",
        "        \"genotype\": \"Characteristics_Genotype\",\n",
        "        \"material\": \"Characteristics_Material_Type\",\n",
        "        \"space flight\": \"Factor_Value_Spaceflight\",\n",
        "        \"age at launch\": \"Characteristics_Age_at_Launch\",\n",
        "        \"sex\": \"Characteristics_Sex\",\n",
        "        \"habitat\": \"Parameter_Value_habitat\",\n",
        "        \"duration\": \"Parameter_Value_duration\",\n",
        "        \"light cycle\": \"Parameter_Value_light_cycle\",\n",
        "        \"enrichment\": \"Parameter_Value_Enrichment_material\",\n",
        "        \"diet\": \"Parameter_Value_diet\",\n",
        "        \"feeding schedule\": \"Parameter_Value_Feeding_Schedule\",\n",
        "        \"euthanasia method\": \"Parameter_Value_Euthanasia_Method\",\n",
        "        \"age at euthanasia\": \"Parameter_Value_Age_at_Euthanasia\",\n",
        "        \"preservation\": \"Parameter_Value_Sample_Preservation_Method\",\n",
        "        \"storage temperature\": \"Parameter_Value_Sample_Storage_Temperature\",\n",
        "        \"body weight at euthanasia\": \"Parameter_Value_Body_Weight_upon_Euthanasia\",\n",
        "        \"euthanasia date\": \"Comment_Euthanasia_Date\",\n",
        "        \"description\": \"Comment_Source_Description\",\n",
        "        \"protocol\": \"Protocol_REF\"\n",
        "    }\n",
        "\n",
        "    # Prefixes that indicate that all information should be returned\n",
        "    info_prefixes = [\"information about\", \"info on\", \"characteristics about\", \"characteristics of\"]\n",
        "\n",
        "    # Helper function to check if the question contains one of the prefixes\n",
        "    def contains_prefix(question, prefixes):\n",
        "        for prefix in prefixes:\n",
        "            if prefix in question:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "\n",
        "    # Iterate over each sample in the dataset\n",
        "    for i in range(len(data[\"Sample_Name\"])):\n",
        "        sample_name = normalize_string(data['Sample_Name'][i])\n",
        "        if f\"experiment {sample_name}\" in question:\n",
        "\n",
        "            if contains_prefix(question, info_prefixes):\n",
        "                for key, column in mapping.items():\n",
        "                    response.append(f\"{key.capitalize()}: {data[column][i]}\")\n",
        "                return \"\\n\".join(response)\n",
        "\n",
        "            else:\n",
        "                for key, column in mapping.items():\n",
        "                    if key in question:\n",
        "                        response.append(f\"{key.capitalize()}: {data[column][i]}\")\n",
        "\n",
        "                return \"\\n\".join(response)\n",
        "\n",
        "    return \"Sorry, I couldn't find any information about the mentioned experiment.\"\n",
        "\n",
        "# Predefined responses for standard queries\n",
        "predefined_responses = {\n",
        "    \"help\": \"I am a virtual assistant to help with information about experiments. You can ask about the name, organism, or any other characteristic of NASA space experiments.\",\n",
        "    \"experiments\": \"Currently, we have various information about NASA space experiments available. Please provide the sample name or any other information you're looking for.\"\n",
        "}\n",
        "\n",
        "def list_experiments():\n",
        "    # Get all unique Sample_Name from the dataset\n",
        "    return data[\"Sample_Name\"].unique()\n",
        "\n",
        "def search_available_experiments(question):\n",
        "    if \"which experiments\" in question.lower() or \"available experiments\" in question.lower():\n",
        "        samples = list_experiments()\n",
        "        return \"The available experiments are:\\n\" + \"\\n\".join(samples)\n",
        "    return None  # Return None if no relevant question is found\n",
        "\n",
        "def search_predefined_response(question):\n",
        "    for key, response in predefined_responses.items():\n",
        "        if key in question.lower():\n",
        "            return response\n",
        "    return None  # Return None if no predefined response is found\n",
        "\n",
        "def interact_with_user():\n",
        "    print(\"Hello! I am a virtual assistant. How can I help you with information about the experiments?\")\n",
        "    while True:\n",
        "        user_question = input(\"\\nYou: \")\n",
        "\n",
        "        if user_question.lower() == 'exit':\n",
        "            print(\"Assistant: Goodbye! If you need more information, feel free to ask.\")\n",
        "            break\n",
        "\n",
        "        experiment_response = search_available_experiments(user_question)\n",
        "        if experiment_response:\n",
        "            print(f\"Assistant:\\n{experiment_response}\\n\")\n",
        "            continue\n",
        "\n",
        "        predefined_response = search_predefined_response(user_question)\n",
        "        if predefined_response:\n",
        "            print(f\"Assistant:\\n{predefined_response}\\n\")\n",
        "            continue\n",
        "\n",
        "        response = fetch_experiment_data(user_question)\n",
        "\n",
        "        print(f\"Assistant:\\n{response}\\n\")\n",
        "\n",
        "\n",
        "interact_with_user()\n"
      ],
      "metadata": {
        "id": "4D9DyZl4xaqC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1dde0e6-ddc1-48ba-c824-1e924dfa08ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nomes das colunas:\n",
            "['Source_File', 'Source_Name', 'Sample_Name', 'Characteristics_Organism', 'Characteristics_Strain', 'Characteristics_Animal_Source', 'Characteristics_Genotype', 'Characteristics_Material_Type', 'Factor_Value_Spaceflight', 'Characteristics_Age_at_Launch', 'Characteristics_Sex', 'Protocol_REF', 'Parameter_Value_habitat', 'Parameter_Value_duration', 'Parameter_Value_light_cycle', 'Parameter_Value_Enrichment_material', 'Parameter_Value_diet', 'Parameter_Value_Feeding_Schedule', 'Protocol_REF_1', 'Parameter_Value_Euthanasia_Method', 'Parameter_Value_Age_at_Euthanasia', 'Parameter_Value_Sample_Preservation_Method', 'Parameter_Value_Sample_Storage_Temperature', 'Parameter_Value_Body_Weight_upon_Euthanasia', 'Comment_Euthanasia_Date', 'Comment_Source_Description']\n",
            "Hello! I am a virtual assistant. How can I help you with information about the experiments?\n",
            "\n",
            "You: info on experiment RR23_R-EDL_FLT_F1_techrep1\n",
            "Assistant:\n",
            "Experiment name: RR-23_F1\n",
            "Sample name: RR23_R-EDL_FLT_F1_techrep1\n",
            "Organism: Mus musculus\n",
            "Strain: C57BL/6J\n",
            "Laboratory: Jackson Laboratory\n",
            "Genotype: Wild Type\n",
            "Material: Right extensor digitorum longus\n",
            "Space flight: Space Flight\n",
            "Age at launch: 16-17 week\n",
            "Sex: Male\n",
            "Habitat: Rodent Flight Hardware (Transporter and Habitat)\n",
            "Duration: 38 day\n",
            "Light cycle: 12 h light: 12 h dark. Lights on at 7:00 GMT\n",
            "Enrichment: Hut\n",
            "Diet: Nutrient Upgraded Rodent Food Bar (NuRFB)\n",
            "Feeding schedule: ad libitum\n",
            "Euthanasia method: Bilateral thoracotomy with sedation\t Cardiac puncture\t Inhalation of Isoflurane\n",
            "Age at euthanasia: 22-23 week\n",
            "Preservation: Liquid Nitrogen\n",
            "Storage temperature: -80 degree Celsius\n",
            "Body weight at euthanasia: 35.9 gram\n",
            "Euthanasia date: 14-jan.-2021\n",
            "Description: Live animal return\n",
            "Protocol: Animal Husbandry\n",
            "\n",
            "\n",
            "You: exit\n",
            "Assistant: Goodbye! If you need more information, feel free to ask.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ojDjaw3xBcJQ",
        "outputId": "b86ddf93-a476-4308-efdc-828c376dda4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Collecting gradio\n",
            "  Downloading gradio-4.44.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0 (from gradio)\n",
            "  Downloading fastapi-0.115.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.3.0 (from gradio)\n",
            "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.7)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.6.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.3)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.31.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi<1.0->gradio)\n",
            "  Downloading starlette-0.38.6-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-4.44.1-py3-none-any.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.0-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
            "Downloading ruff-0.6.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading uvicorn-0.31.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.38.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, tomlkit, semantic-version, ruff, python-multipart, orjson, h11, ffmpy, aiofiles, uvicorn, starlette, httpcore, httpx, fastapi, gradio-client, gradio\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.0 ffmpy-0.4.0 gradio-4.44.1 gradio-client-1.3.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 orjson-3.10.7 pydub-0.25.1 python-multipart-0.0.12 ruff-0.6.9 semantic-version-2.10.0 starlette-0.38.6 tomlkit-0.12.0 uvicorn-0.31.0 websockets-12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import unicodedata\n",
        "import re\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "\n",
        "# Lê o arquivo CSV\n",
        "file_path = '/content/testerelatorios.csv'  # Substitua pelo caminho do seu arquivo CSV\n",
        "data = pd.read_csv(file_path, delimiter=',')\n",
        "\n",
        "# Carrega o modelo e o tokenizador\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "def query_gpt2(question):\n",
        "    input_ids = tokenizer.encode(question, return_tensors='pt')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(input_ids, max_length=50)\n",
        "\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return response.strip()\n",
        "\n",
        "def normalize_string(text):\n",
        "    text = text.lower().strip()\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')  # Remove accents\n",
        "    return text\n",
        "\n",
        "def remove_duplicate_spaces(text):\n",
        "    return re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "def fetch_experiment_data(question_input):\n",
        "    response = []\n",
        "\n",
        "    question = remove_duplicate_spaces(normalize_string(question_input))\n",
        "\n",
        "    mapping = {\n",
        "        \"experiment name\": \"Source_Name\",\n",
        "        \"sample name\": \"Sample_Name\",\n",
        "        \"organism\": \"Characteristics_Organism\",\n",
        "        \"strain\": \"Characteristics_Strain\",\n",
        "        \"laboratory\": \"Characteristics_Animal_Source\",\n",
        "        \"genotype\": \"Characteristics_Genotype\",\n",
        "        \"material\": \"Characteristics_Material_Type\",\n",
        "        \"space flight\": \"Factor_Value_Spaceflight\",\n",
        "        \"age at launch\": \"Characteristics_Age_at_Launch\",\n",
        "        \"sex\": \"Characteristics_Sex\",\n",
        "        \"habitat\": \"Parameter_Value_habitat\",\n",
        "        \"duration\": \"Parameter_Value_duration\",\n",
        "        \"light cycle\": \"Parameter_Value_light_cycle\",\n",
        "        \"enrichment\": \"Parameter_Value_Enrichment_material\",\n",
        "        \"diet\": \"Parameter_Value_diet\",\n",
        "        \"feeding schedule\": \"Parameter_Value_Feeding_Schedule\",\n",
        "        \"euthanasia method\": \"Parameter_Value_Euthanasia_Method\",\n",
        "        \"age at euthanasia\": \"Parameter_Value_Age_at_Euthanasia\",\n",
        "        \"preservation\": \"Parameter_Value_Sample_Preservation_Method\",\n",
        "        \"storage temperature\": \"Parameter_Value_Sample_Storage_Temperature\",\n",
        "        \"body weight at euthanasia\": \"Parameter_Value_Body_Weight_upon_Euthanasia\",\n",
        "        \"euthanasia date\": \"Comment_Euthanasia_Date\",\n",
        "        \"description\": \"Comment_Source_Description\",\n",
        "        \"protocol\": \"Protocol_REF\"\n",
        "    }\n",
        "\n",
        "    info_prefixes = [\"information about\", \"info on\", \"characteristics about\", \"characteristics of\"]\n",
        "\n",
        "    def contains_prefix(question, prefixes):\n",
        "        for prefix in prefixes:\n",
        "            if prefix in question:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    for i in range(len(data[\"Sample_Name\"])):\n",
        "        sample_name = normalize_string(data['Sample_Name'][i])\n",
        "        if f\"experiment {sample_name}\" in question:\n",
        "\n",
        "            if contains_prefix(question, info_prefixes):\n",
        "                for key, column in mapping.items():\n",
        "                    response.append(f\"{key.capitalize()}: {data[column][i]}\")\n",
        "                return \"\\n\".join(response)\n",
        "\n",
        "            else:\n",
        "                for key, column in mapping.items():\n",
        "                    if key in question:\n",
        "                        response.append(f\"{key.capitalize()}: {data[column][i]}\")\n",
        "\n",
        "                return \"\\n\".join(response)\n",
        "\n",
        "    return \"Sorry, I couldn't find any information about the mentioned experiment.\"\n",
        "\n",
        "def chatbot_response(user_input):\n",
        "    experiment_response = search_available_experiments(user_input)\n",
        "    if experiment_response:\n",
        "        return experiment_response\n",
        "\n",
        "    predefined_response = search_predefined_response(user_input)\n",
        "    if predefined_response:\n",
        "        return predefined_response\n",
        "\n",
        "    response = fetch_experiment_data(user_input)\n",
        "    return response\n",
        "\n",
        "# Predefined responses for standard queries\n",
        "predefined_responses = {\n",
        "    \"help\": \"I am a virtual assistant to help with information about experiments. You can ask about the name, organism, or any other characteristic of NASA space experiments.\",\n",
        "    \"experiments\": \"Currently, we have various information about NASA space experiments available. Please provide the sample name or any other information you're looking for.\"\n",
        "}\n",
        "\n",
        "def list_experiments():\n",
        "    return data[\"Sample_Name\"].unique()\n",
        "\n",
        "def search_available_experiments(question):\n",
        "    if \"which experiments\" in question.lower() or \"available experiments\" in question.lower():\n",
        "        samples = list_experiments()\n",
        "        return \"The available experiments are:\\n\" + \"\\n\".join(samples)\n",
        "    return None\n",
        "\n",
        "def search_predefined_response(question):\n",
        "    for key, response in predefined_responses.items():\n",
        "        if key in question.lower():\n",
        "            return response\n",
        "    return None\n",
        "\n",
        "# Gradio Interface\n",
        "iface = gr.Interface(fn=chatbot_response,\n",
        "                     inputs=\"text\",\n",
        "                     outputs=\"text\",\n",
        "                     title=\"NASA Experiment Chatbot\",\n",
        "                     description=\"Ask about NASA space experiments and get information based on our database.\")\n",
        "\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "id": "RC8V5ttAFOTx",
        "outputId": "dbe1a2f7-76d0-411d-cf69-a34670deb62d"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://2845adca7c47668d01.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2845adca7c47668d01.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9kDBZRUeNkZ3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}